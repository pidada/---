{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 剪枝\n",
    "#### 为什么要剪枝\n",
    ">决策树算法在生成的过程中，利用递归的方式产生决策树，直到不能继续下去，容易造成对现有的训练数据有很好的分类，但是对未知数据分类不准确，造成了过拟合的现象。\n",
    "\n",
    "在决策树学习中，将已经生成树进行简化的过程称之为剪枝`prunning`。简单地说：\n",
    "- 去掉某些叶结点或者子树\n",
    "- 将上述叶节点的父节点作为新的叶结点\n",
    "\n",
    "#### 剪枝过程\n",
    "决策树的剪枝过程，往往就是通过最小化决策树整体的损失函数或者代价函数来实现。损失函数定义为$$C_{\\alpha}(T)=\\sum _{t=1}^{|T|}N_tH_t(T)+\\alpha|T|$$经验熵为$$H_t(T)=-\\sum_k\\frac{N_{tk}}{N_t}log\\frac{N_{tk}}{N_t}$$在损失函数中，通常将右端的第一项记为：$$C(T)=\\sum _{t=1}^{|T|}N_tH_t(T)=-\\sum_{t=1}^{|T|}\\sum_{k=1}^{K}N_{tk}log\\frac{N_{tk}}{N_t}$$有$$C_{\\alpha}(T)=C(T)+\\alpha|T|$$\n",
    "- $C(T)$表示模型训练数据的预测误差，模型和训练数据之间的拟合程度\n",
    "- $|T|$表示模型的复杂程度\n",
    "- $\\alpha$控制两者之间的影响\n",
    "  - $\\alpha$大，模型简单\n",
    "  - $\\alpha$小，模型复杂\n",
    "  - $\\alpha=0$表示只考虑模型和训练数据之间的拟合程度，不考虑模型的复杂度\n",
    "  \n",
    ">剪枝的过程就，$\\alpha$确定时，选择损失函数最小的模型，即损失函数最小的子树。\n",
    "  \n",
    "#### 决策树生成和剪枝\n",
    "- 生成：\n",
    "    - 通过提高信息增益和信息增益率对训练数据进行更好的拟合；\n",
    "    - 学习局部的模型\n",
    "- 剪枝：\n",
    "    - 通过优化损失函数来减小模型的复杂度；\n",
    "    - 学习整体的过程\n",
    "    - 利用损失函数最小原则进行剪枝就是用正则化的极大似然估计来进行模型选择\n",
    "\n",
    "#### 剪枝算法\n",
    "输入：生成算法产生的整个数$T$，参数$\\alpha$\n",
    "\n",
    "输出：修剪后的子树$T_{\\alpha}$\n",
    "\n",
    "步骤：\n",
    "- 计算每个节点的经验熵\n",
    "- 递归地从树的叶结向上回缩\n",
    "- 满足回缩前后整体树的$$C_{\\alpha}{(T_A)}\\leq C_{\\alpha}{(T_B)}$$则进行剪枝，即原来的父节点变成新的叶结点\n",
    "- 重复上述操作，直到不能继续为止，得到最小损失函数的子树$T_{\\alpha}$\n",
    "\n",
    "### CART算法\n",
    "`CART`算法由特征选择、树的生成及剪枝组成，可以用于回归也可以用于分类。`CART`假设决策树是二叉树，内部节点特征的取值为\"是\"和\"否\"，左边取值为\"是\"的分支，右边为\"否\"的分支，进行递归地二分每个特征，算法分为两步：\n",
    "- 决策树生成：基于训练数据集生成决策树，生成的决策树尽量大；\n",
    "- 决策树剪枝：用验证数据集对已生成的树进行剪枝，并且选择最优树，用最小化损失函数作为剪枝的标准\n",
    "\n",
    "决策树的生成就是递归地创建二叉树的过程。最小化准则：\n",
    "- 回归：平方误差最小化\n",
    "- 分类：基尼系数最小化\n",
    "\n",
    "#### 回归树\n",
    "假设输入和输出变量分别是X和Y，并且Y是连续变量，在数据集$D=\\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\\}$上，如果将输入空间划分为M个单元$R_1,R_2,...,R_M$，每个单元对应的输出值为$c_m$，回归树模型为：$$f(x)=\\sum_{m=1}^{M}c_mI(x\\in{R_m})$$平方误差为$$\\sum_{x_i\\in R_m}(y_i-f(x_i))^2$$用来表示回归树对于训练数据的预测误差\n",
    "#### 分类树\n",
    "分类数用基尼系数选择最优特征，同时决定该特征的最优二值分切分点。分类问题中，假设有K个类，样本点属于第k类的概率为$p_k$，概率分布的基尼质数定义为：\n",
    "$$\n",
    "\\begin{align}\n",
    "Gini(p)\n",
    "& =\\sum_{k=1}^{K}p_k(1-p_k) \\\\\n",
    "& =\\sum_{k=1}^{K}p_k - \\sum_{k=1}^{K}p_k^2 \\\\\n",
    "& = 1- \\sum_{k=1}^{K}p_k^2\n",
    "\\end{align}\n",
    "$$\n",
    "对于二分类问题，若属于第一个类的概率为p，概率分布的基尼系数为：$$Gini(D)=p(1-p)+(1-p)(1-(1-p))=2p(1-p)$$对给定的样本集合D，基尼系数为$$Gini(D)=1-\\sum_{k=1}^{K}(\\frac {|C_k|} {|D|})^2$$其中K是类的个数，$C_k$是%D%中属于第$k$类的样本子集。\n",
    "\n",
    "**基尼系数表示样本集合的不确定性，基尼系数越大，样本集合的不确定性就越大**\n",
    "\n",
    "#### CART算法\n",
    "输入：训练数据集D，停止计算的条件\n",
    "输出：CART决策树\n",
    "\n",
    "具体构建决策树的步骤为：\n",
    "- 设节点的数据集为D，计算现有特征对该数据集的基尼系数\n",
    "- 在所有的特征A以及它们可能的切分点中，选择基尼系数最小的特征及对应的切分点作为最佳选择。\n",
    "- 从现有节点中生成两个子节点，将训练数据集分配到两个子节点中去\n",
    "- 递归地调用上面两个步骤，直到满足停止条件\n",
    "- 生成CART树\n",
    "\n",
    "**停止条件：节点中样本数小于阈值，或者样本集的基尼系数小于阈值，或者没有更多特征**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
