{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知机Perceptron\n",
    "#### 导读\n",
    ">感知机是二分类的线性分类模型，输入是实例的特征向量（每个属性），输出是实例的类别。感知机对应于输入空间中将实例划分为正负两类的分离超平面，属于判别模型。\n",
    "- 目的：找出将训练数据进行线性划分的分离超平面\n",
    "- 导入基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求出最小值\n",
    "- 有原始形式和对偶形式两种\n",
    "- 感知机是种线性分类模型，属于判别模型。\n",
    "\n",
    "#### 定义\n",
    "现在假设输入空间是$X\\subseteq{R^n}$，输出空间$Y=\\{+1, -1\\}$。输入$x\\in X$表示实例的特征向量，输出$y\\in Y$表示实例的类别，其中输入到输出的函数表示如下：$$f(x)=sign(w.x+b)$$，称为感知机\n",
    "- w，b称为感知机模型参数；w称为权值或者权值向量；$b\\in R$称为偏置`bias`。w.x表示二者的内机，`sign`表示符号函数：\n",
    "$$\n",
    "sign(x)=\n",
    "\\begin{cases}\n",
    "+1, & {x \\geq 0} \\\\\n",
    "-1, & {x \\leq 0}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "感知机的几何解释为线性方程：$$w.x+b=0$$。特征空间$R^n$对应一个超平面S，其中w是超平面的法向量，b是超平面的截距。超平面将特征空间划分为两个部分。位于两部分的点分为正负两个类别。正类为+1，父类为-1。\n",
    "\n",
    "#### 策略\n",
    ">给定一个数据集$$T=\\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\\}$$其中$x_i$是输入空间实例的特征向量，$y_i$是实例的输出类别，如果存在超平面S满足将数据集的正负实例完全分开，即满足：当$w.x_i+b>0$ ,有$y_i=+1$；当$w.x_i+b<0$ ,有$y_i=-1$。此时，将所有的数据分为线性可分数据集，否则称为线性不可分。\\\n",
    "- 感知机中的损失函数误分类点到超平面S的总距离，输入空间中任意一个点$x_0$到距离是$$\\frac{1}{||w||}{(w.x_0+b)}$$，其中||w||表示w的$L_2$范数，即：$$||w||=\\sqrt{w_1^2+w_2^2+...+w_n^2}$$。对于误分类的点，总有如下结论：$$-y_i(w.x_i+b)>0$$。因为在误分类的时候，每个实例点满足：当$w.x_i+b>0$ ,有$y_i=-1$；当$w.x_i+b<0$ ,有$y_i=+1$。因此，**误分类的点到超平面的距离**：$$-\\frac{1}{||w||}{y_i}{(w.x_0+b)}$$，假设超平面中的所有误分类的集合M，所有误分类的点到S的总距离为：$$-\\frac{1}{||w||}\\sum_{x_i \\in M}{y_i}{(w.x_0+b)}$$，不考虑$-\\frac{1}{||w||}$得到了感知机$f(x)=sign(w.x+b)$的损失函数为：$$L(w,b)=\\sum_{x_i \\in M}{y_i}{(w.x_0+b)}$$\n",
    "- M是误分类点的集合\n",
    "- 损失函数就是感知机学习的经验风险函数\n",
    "- 梯度只是向量，代表下降的方向\n",
    "- 通过学习率来控制下降的大小\n",
    "- 如果没有误分类点，损失函数是0\n",
    "- 误分类点越少，总距离越小，损失函数越小\n",
    "- L是参数(w,b)的连续可导函数\n",
    "\n",
    "#### 算法\n",
    "##### 原始形式\n",
    "感知机学习算法的最优化问题就是求解损失函数的最小值，即：$$\\mathop {\\min \\limits_{w,b}L(w,b)}=\\sum_{x_i \\in M}{y_i}{(w.x_0+b)}$$。感知机的算法是误分类驱动的，具体采用的是随机梯度下降法(stochastic gradient descent)，大致过程如下：\n",
    "- 选取任意的超平面$w_0,b_0$\n",
    "- 利用梯度下降方法去不断地极小化目标损失函数L(w, b)\n",
    "- 在不断极小化的过程中，不是一次性使用M中所有误分类点进行梯度下降，**而是一次随机选取一个误分类点进行梯度下降。**\n",
    "- 损失函数$L(w, b)$的梯度由如下的式子决定：$$\\nabla_wL{(w,b)}=-\\sum_{x_i \\in M}{y_ix_i}$$\n",
    "$$\\nabla_bL{(w,b)}=-\\sum_{x_i \\in M}{y_i}$$\n",
    "每次随机选取一个误分类点$(x_i, y_i)$，对$w, b$进行更新：$$w\\gets{w+\\eta{y_ix_i}}$$$$b\\gets{b+\\eta{y_i}}$$其中$\\eta\\in{(0,1]}$表示学习率或者步长。通过不断地迭代损失函数$L(w,b)$使其不断的减小，直至为0\n",
    "\n",
    "#### 算法描述\n",
    "输入：训练数据集$T=\\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\\}$，其中$x_i\\in X=R^n$,$y_i \\in Y=\\{-1, +1\\}, i=1,2,....,N$；学习率$0 < \\eta \\leq 1$\n",
    "\n",
    "输出：w,b；感知机模型$f(x)=sign(w.x+b)$\n",
    "- 选取初值$w_0,b_0$，一般是0\n",
    "- 在训练集中选取数据$(x_i,y_i)$\n",
    "- 如果存在误分类点，即满足$-y_i(w.x_i+b)\\geq {0}$或者y_i(w.x_i+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
