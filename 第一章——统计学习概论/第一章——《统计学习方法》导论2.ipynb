{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型评估和选择\n",
    "#### 训练误差和测试误差\n",
    "假设学习模型是$Y=\\hat{f}(X)$，训练误差是模型关于训练数据集的平均损失：$R_{emp}(\\hat{f})$$$R_{emp}(\\hat{f})=\\frac{1}{N}\\sum_{i=1}^NL(y_i,\\hat{f}(x_i))$$其中N是训练样本的容量\n",
    "\n",
    "训练误差是关于数据集的平均损失：$$e_{test}=\\frac{1}{N'}\\sum_{i=1}^{N'}L(y_i,\\hat{f}(x_i))$$其中$N'是测试样本的容量$\n",
    "\n",
    "当损失函数是0-1损失时，测试误差就变成了常见的测试数据集上的误差率`error rate`$$e_{test}=\\frac{1}{N'}\\sum_{i=1}^{N'}I(y_i \\neq {\\hat{f}}(x_i))$$\n",
    "I是指示函数，即$y\\neq{\\hat{f}}(x)$时为1，否则为0；相应的测试集上的准确率为$$r_{test}=\\frac{1}{N'}\\sum_{i=1}^{N'}I(y_i = {\\hat{f}}(x_i))$$\n",
    "明显的：$$r_{test} + e_{test}=1$$\n",
    "\n",
    "\n",
    "**总结**\n",
    "- 测试误差反应了学习方法对未知的测试数据集的预测能力\n",
    "- 通过学习方法对未知数据的预测能力称为**泛化能力**\n",
    "\n",
    "#### 过拟合与模型选择\n",
    "过拟合：一味地追求提高对训练数据的预测能力，所选模型的复杂度会比真实模型高，这种现象称之为`过拟合`。\n",
    "- 过拟合是指学习时选择的**参数过多**\n",
    "- 过拟合对已知数据能够很好的判断，但是对未知数据预测的效果很差\n",
    "- 模型选择的目的在于避免过拟合并且提高模型的预测能力\n",
    "\n",
    "栗子：对M次多项式进行拟合$$f_M(x,w)=w_0+w_1x+w_2x^2+...+w_Mx^M=\\sum_{j=0}^Mw_jx^j$$\n",
    "解决办法：\n",
    "- 确定模型的复杂度，即多项式的次数\n",
    "- 在给定的模型复杂度下，根据经验风险最小的策略，求解参数，即多项式的系数\n",
    "- 经验风险最小化$$L(w)=\\frac{1}{2}\\sum_{i=1}^{N}(f(x_i,w)-y_i)^2$$\n",
    "- 损失函数是平方损失，系数$\\frac{1}{2}$是为了后续的求导计算。\n",
    "\n",
    "1. 在多项式拟合中，训练误差随着多项式系数即模型复杂度的增加而减小\n",
    "2. 测试误差随着模型复杂度的增加先减小后增加\n",
    "3. 优化的目的：使得测试误差达到最小\n",
    "4. 当模型的复杂度过大，就会出现过拟合的现象，使用`正则化和交叉验证`来解决\n",
    "\n",
    "#### 正则化\n",
    "**模型选择的典型方法是正则化**。正则化是结构风险最小化策略的实现，在经验风险的基础上加上了一个正则项`regularrizer`或者罚项`penalty term`。正则化的一般形式$$\\mathop{min}\\limits_{f\\in \\Gamma}\\frac{1}{N}\\sum_{i=1}^NL(y_i,f(x_i))+\\lambda J()$$\n",
    "- 第一项是经验风险\n",
    "- 第二项是正则化项\n",
    "- 两者构成了结构风险\n",
    "- $\\lambda$是正则化系数\n",
    "- 正则化项具有不同的形式：比如回归问题中，参数向量的$L_1$范数$$L(w)=\\frac{1}{N}\\sum_{i=1}^N(f(x_i;w)-y_i)^2+\\lambda ||w||_1$$或者表示成参数向量的$L_2$范数$$L(w)=\\frac{1}{N}\\sum_{i=1}^N(f(x_i;w)-y_i)^2+{\\frac{\\lambda}{2}||w||^2}$$\n",
    "\n",
    "\n",
    "**正则化作用：选择经验风险和模型复杂度同时较小的模型**\n",
    "\n",
    "#### 交叉验证\n",
    "##### 普通模型选择方法\n",
    "进行模型选择的`一般做法`是指将数据集分成三个部分：\n",
    "- 训练集`training  set`\n",
    "    - 作用是训练模型\n",
    "- 验证集`validation set`\n",
    "    - 作用是用于模型的选择；一般数据足够多\n",
    "- 测试集`test set`\n",
    "    - 对学习方法的评估\n",
    "    \n",
    "**在学习到不同复杂度的模型中，选择对验证集有最小预测误差的模型**\n",
    "##### 简单交叉验证\n",
    "交叉验证`cross validation`的做法是数据分成两部分：\n",
    "- 训练集 70%\n",
    "- 测试集 30%\n",
    "\n",
    "通过训练集在不同的条件下进行模型的训练，从而得到模型，再把测试集数据放入模型进行评估，选择出测试误差最小的模型\n",
    "\n",
    "##### S折交叉验证\n",
    "`S-fold cross validation` 的做法是：\n",
    "- 将数据随机分成`S`个互不相交、大小相同的子集\n",
    "- 利用S-1个子集进行训练\n",
    "- 利用剩下的子集进行测试\n",
    "- 对`S`中选择重复进行\n",
    "- 最后选择`S`次评测中测试误差最小的模型\n",
    "\n",
    "##### 留一交叉验证\n",
    "`S`折交叉验证的特殊情形是`S=N`，变成`留一交叉验证 leave-ont-out cross validation`，往往是在**数据缺乏的情况下使用**\n",
    "\n",
    "\n",
    "### 泛化能力\n",
    "#### 什么是泛化误差\n",
    "学习方法的泛化能力是指由学习方法得到的模型对未知数据的预测能力，是学习方法重要的性质。通常是采用**通过测试误差来评估学习方法的泛化能力。**缺陷是\n",
    "- 过于依赖于测试数据\n",
    "- 测试数据是有限的，评估结果未必可靠\n",
    "\n",
    "如果学到的模型是$\\hat{f}$，用该模型对未知数据预测的误差称为泛化误差`generalization error`，通过泛化误差来反映学习的泛化能力：\n",
    "$$\n",
    "\\begin{align}\n",
    "R_{exp}(f)\n",
    "& = E_P[L(Y,\\hat{f}(X))]\\\\\n",
    "& = \\int_{X{\\times}Y}{L(y, \\hat{f}(x))P(x,y)}{\\rm d}x{\\rm d}y\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**泛化误差就是所学习到的模型的期望风险**\n",
    "\n",
    "#### 泛化误差上界GEB\n",
    ">泛化能力分析往往是通过研究比较泛化误差的概率上界来实现的，称之为泛化误差上界 `generalization error bound`。泛化误差两个特质：\n",
    "- GEB是样本容量的函数，样本容量增减，GEB趋于0；\n",
    "- GEB是假设空间容量的函数，假设空间容量越大，模型越难学，GEB越大\n",
    "\n",
    "#### 二分类问题的GEB讨论\n",
    "- 已知训练集：$$T={(x_1,y_1), (x_2,y_2),...(x_i,y_i),...,(x_N,y_N)}$$N是样本容量，$T$是从联合概率分布$P(X,Y)$独立同分布产生的，$X\\in R^n，Y\\in\\{-1,+1\\}$\n",
    "- 假设空间是函数的集合$\\Gamma = \\{f_1, f_2, ..., f_d\\}$，d是函数的个数。\n",
    "- 损失函数是0-1损失\n",
    "- 假设f是从$\\Gamma$中选择出来，则f的\n",
    "期望风险是$$R(f)=E[L(Y,f(X))]$$\n",
    "经验风险是$$\\hat R(f)=\\frac{1}N\\sum_{i=1}^NL(y_i, f(x_i))$$\n",
    "经验风险最小化函数是$$f_N=arg \\mathop{min}\\limits_{f\\in \\Gamma}\\hat R(f)$$\n",
    "泛化能力$$R(f_N)=E[L(Y,f_N(X))]$$\n",
    "其中$f_N$依赖于样本容量N。\n",
    "\n",
    "\n",
    "**泛化误差上界定理**\n",
    "对于二分裂问题，当假设空间是有限个函数的集合，$\\Gamma = \\{f_1, f_2, ..., f_d\\}$，d是函数的个数，任意的$f\\in \\Gamma$，至少有以概率$1-\\delta, 0 < \\delta < 1$使得如下式子成立：$$R(f) \\leq \\hat R(f)+\\epsilon (d,N,\\delta)$$，其中$$\\epsilon (d,N,\\delta)=\\sqrt {\\frac{1}N(logd+log \\frac {1}\\delta)}$$\n",
    "- 等式的左边是泛化误差，右端是泛化误差上界\n",
    "- 在泛化误差上界中第一项是训练误差，训练误差越小，泛化误差越小\n",
    "- 第二项中$\\epsilon (d,N,\\delta)$是样本容量N的单调递减函数，当N趋于无穷大时$\\delta$趋于0\n",
    "- 同时第二项也和假设函数个数d相关；假设空间$\\Gamma$包含的函数越多，其值越大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
