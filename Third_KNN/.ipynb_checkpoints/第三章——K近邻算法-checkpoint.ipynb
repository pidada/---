{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN导读\n",
    ">k-近邻算法（k-nearest neighbor, k-NN）是一种基本分类和回归的算法。k近邻算法中的输入为实例的特征向量，输出为实例的类别，类别可以有多类。算法主要思想：\n",
    "- 给定一个训练集的数据，实例的类别已定\n",
    "- 对于新的实例，根据`k`个最近邻的训练实例的类别，经投票表决等方式进行预测\n",
    "- 算法不具有显式的学习过程，实际上利用训练集对特征向量空间进行划分。\n",
    "\n",
    "### KNN三要素\n",
    "- k的选择：k值如何选择？越大越好吗？奇偶性如何？经验值是多少？\n",
    "- 距离度量：选择什么距离来进行度量新实例和训练集上点的距离？\n",
    "- 分类决策规则：选择怎样的规则来对距离进行分类，从而判断新实例属于哪个类？\n",
    "\n",
    "### k近邻算法\n",
    ">直观解释：给定一个训练数据集，对于新输入的实例，在训练集数据中找出和该实例最邻近的k个实例。这k个实例中的多数属于某个类，就将新实例划分为这个类别。\n",
    "输入训练数据集：\n",
    "$$T=\\{(x_1,y_1),(x_2,y_2),...(x_i,y_i)....(x_N,y_N)\\}$$\n",
    "其中，x<sub>i</sub>为实例特征向量，y<sub>i</sub>为实例的类别；i=1,2,3,...N。\n",
    "输出：实例x所属的类别y\n",
    "\n",
    "- 根据给定的距离度量，在训练集T中找出与x最近邻的k个点，涵盖这个k个点的x的邻域记作：N<sub>k</sub>(x)\n",
    "- 在邻域N<sub>k</sub>(x)中根据分类规则决定x的类别y\n",
    "$$y = \\mathop {arg max}\\limits_{c_j}\\sum_{x_i\\in{N_k(x)}}I(y_i=c_j), i=1,2...,N;j=1,2,...K$$\n",
    "\n",
    "上式中，I为指示函数，即当：y<sub>i</sub>=c<sub>j</sub>是为1，不等则为0\n",
    "\n",
    "- k=1称之为最近邻算法。对于输入的新实例，将训练集中离x**最近点**的所属类作为x的类别\n",
    "\n",
    "### k近邻模型\n",
    "k近邻算法的模型主要是有三个要素:\n",
    "- 距离度量\n",
    "- k值的选择\n",
    "- 分类决策规则的规定\n",
    "\n",
    "#### 距离度量\n",
    ">特征空间中两个实例点的距离是两个实例点相似度的反应。k近邻模型的特征空间一般是n维实数向量空间$R^n$。一般使用的欧式距离，也可以是其他距离，如：$L_p$距离，或者`Minkowski`距离。\n",
    "\n",
    "假设特征空间$X$是n维实数向量空间$R^n$，${x_i,x_j}\\in X$，其中$x_i=(x_i^{(1)},x_i^{(2)},....,x_i^{(n)}$，\n",
    "$x_j=(x_j^{(1)},x_j^{(2)},....,x_j^{(n)}$\n",
    "\n",
    "$x_i,x_j$的$L_p$的距离定义为$$L_p(x_i,x_j)=(\\sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^p)^\\frac{1}{p}$$\n",
    "\n",
    "规定：$p\\geq1$\n",
    "\n",
    "- 当$p=2$时，即为欧式距离，即：$$L_2(x_i,x_j)=(\\sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^2)^\\frac{1}{2}$$\n",
    "- 当$p=2$时，即为欧式距离，即：$$L_1(x_i,x_j)=(\\sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|$$\n",
    "- 当$p$趋于无穷，它是各个坐标距离的最大值：$$L_{\\infty}(x_i,x_j)=\\mathop {max}\\limits_{l}|x_i^{(l)}-x_j^{(l)}|$$\n",
    "\n",
    "#### k值选择\n",
    "1. k值较小\n",
    "- 用叫小的邻域中的实例进行预测\n",
    "- 近似误差减小，估计误差增大\n",
    "- 预测结果对近邻的实例点非常敏感；如果近邻点恰好是噪声，预测出错\n",
    "\n",
    "2. k值较大\n",
    "- 用较大的邻域中的实例点进行预测\n",
    "- 减少学习的估计误差，但是近似误差增大\n",
    "- 与输入实例较远的点的训练实例也会起预测作用\n",
    "- k值增大意味着整个模型变得简单\n",
    "\n",
    "**k值一般选取较小的值，一般是奇数值；通过交叉验证来选取最优的k值**\n",
    "\n",
    "#### 分类决策的规则\n",
    "k近邻法中分类决策通常采取的是**多数表决**，即输入实例的k个近邻的训练实例中的多数列决定输入实例的类。如果分类的损失函数是0-1分类，分类函数是$$f:R^n--->\\{c_1,...,c_n\\}$$，误分类的概率是$$P(Y\\neq {f(X)})=1-P(Y=f(X))$$，多数表决规则等价于经验风险最小化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
