{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start read data\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'./Input/train.csv' does not exist: b'./Input/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-71c7c19fd3ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[0mtime_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;31m# 没有这两行是不敢跑的, 300行 0.58， 全量样本跑结果大概0.62\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-71c7c19fd3ac>\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(path_)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \"\"\"\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m     \u001b[0mraw_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\venv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\venv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\venv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\venv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\venv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'./Input/train.csv' does not exist: b'./Input/train.csv'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "#todo: review code, check score function.\n",
    "\n",
    "\n",
    "class LogisticRegression(object):\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "\n",
    "                 learning_step = 0.0001,\n",
    "\n",
    "                 epsilon=0.001,\n",
    "\n",
    "                 n_iter=1500):\n",
    "\n",
    "        self.learning_step = learning_step\n",
    "\n",
    "        self.epsilon_ = epsilon\n",
    "\n",
    "        self.n_iter_ = n_iter\n",
    "\n",
    "        self.coef_ = np.array([])\n",
    "\n",
    "        self.cols_ = []\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, x_, y_):\n",
    "\n",
    "        return self.gradient_descent(x_, y_, epsilon_=self.epsilon_, n_iter=self.n_iter_)\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, x_):\n",
    "\n",
    "        # print(self.cols_, self.coef_)\n",
    "\n",
    "        rst = np.array([self.cols_[idx] for idx in [np.argmax(rst) for rst in sigmoid(np.dot(x_, self.coef_.T))]])\n",
    "\n",
    "        return rst\n",
    "\n",
    "\n",
    "\n",
    "    def gradient_descent(self, x_, y_, epsilon_=0.00001, n_iter=1500):\n",
    "\n",
    "        n = x_.shape[len(x_.shape)-1]\n",
    "\n",
    "        # f_his = []\n",
    "\n",
    "        # one-hot encoding\n",
    "\n",
    "        y_ = pd.get_dummies(y_)\n",
    "\n",
    "        w_ = np.array([])\n",
    "\n",
    "        print(n, y_.shape, y_.columns)\n",
    "\n",
    "\n",
    "\n",
    "        # OvR for multiclass N个分类器 ck vs rest\n",
    "\n",
    "        for ck in np.arange(y_.shape[1]):\n",
    "\n",
    "            wck_ = np.zeros(n)\n",
    "\n",
    "            # k = 0\n",
    "\n",
    "            for k in np.arange(n_iter):\n",
    "\n",
    "                # f_xk = self.f(x_, y_.values[:, ck], wck_)\n",
    "\n",
    "                g_k = self.g(x_, y_.values[:, ck], wck_)\n",
    "\n",
    "\n",
    "\n",
    "                if np.average(g_k*g_k) < epsilon_:\n",
    "\n",
    "                    w_ = wck_ if w_.size == 0 else np.vstack([w_, wck_])\n",
    "\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "\n",
    "                    p_k = -g_k\n",
    "\n",
    "                lambda_k = 0.0000001  # TODO: 更新算法\n",
    "\n",
    "                wck_ = wck_ + lambda_k*p_k\n",
    "\n",
    "                # f_his.append(f_xk)\n",
    "\n",
    "            if k == n_iter-1:\n",
    "\n",
    "                w_ = wck_ if w_.size == 0 else np.vstack([w_, wck_])\n",
    "\n",
    "            print(\"progress: %d done\" % ck)\n",
    "\n",
    "        self.coef_ = w_\n",
    "\n",
    "        self.cols_ = y_.columns.tolist()\n",
    "\n",
    "        return self.coef_, self.cols_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def f(x_, y_, w_):\n",
    "\n",
    "    # Logistic Regression Loss\n",
    "\n",
    "    m = y_.size\n",
    "\n",
    "    rst_ = -(1 / m) * np.sum(np.dot(x_, w_) * y_ - np.log(1 + np.exp(np.dot(x_, w_))))\n",
    "\n",
    "    return rst_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def g(x_, y_, w_):\n",
    "\n",
    "    m = y_.size\n",
    "\n",
    "    # y is one-hot form\n",
    "\n",
    "    # print(y_)\n",
    "\n",
    "    # probe here and check\n",
    "\n",
    "    # x_.shape, y_.shape, w_.shape\n",
    "\n",
    "    # np.dot(x_, w_).shape\n",
    "\n",
    "    # sigmoid(np.dot(x_, w_)).shape\n",
    "\n",
    "    # np.dot(x_.T, y_ * sigmoid(np.dot(x_, w_))).shape\n",
    "\n",
    "    rst_ = -(1 / m) * np.dot(x_.T, y_-sigmoid(np.dot(x_, w_)))\n",
    "\n",
    "    return rst_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(x_):\n",
    "\n",
    "    p = np.exp(x_)\n",
    "\n",
    "    p = p / (1 + p)\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_data(path_='./Input/train.csv'):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    data size is 28x28, 784\n",
    "\n",
    "    :param path_:\n",
    "\n",
    "    :return:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    raw_data = pd.read_csv(path_)\n",
    "\n",
    "    y = raw_data[\"label\"].values\n",
    "\n",
    "    del raw_data[\"label\"]\n",
    "\n",
    "    X = raw_data.values\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print('Start read data')\n",
    "\n",
    "    time_1 = time.time()\n",
    "\n",
    "    X, y = load_data()\n",
    "\n",
    "    # 没有这两行是不敢跑的, 300行 0.58， 全量样本跑结果大概0.62\n",
    "\n",
    "    X = X[:300]\n",
    "\n",
    "    y = y[:300]\n",
    "\n",
    "    train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.33, random_state=2018)\n",
    "\n",
    "    print(set(train_y), set(test_y))\n",
    "\n",
    "    time_2 = time.time()\n",
    "\n",
    "    print('read data cost ', time_2 - time_1, ' second', '\\n')\n",
    "\n",
    "\n",
    "\n",
    "    print('Start training')\n",
    "\n",
    "    clf = LogisticRegression()\n",
    "\n",
    "    clf.f = f\n",
    "\n",
    "    clf.g = g\n",
    "\n",
    "    clf.fit(train_x, train_y)\n",
    "\n",
    "    time_3 = time.time()\n",
    "\n",
    "    print('training cost ', time_3 - time_2, ' second', '\\n')\n",
    "\n",
    "\n",
    "\n",
    "    print('Start predicting')\n",
    "\n",
    "    test_predict = clf.predict(test_x)\n",
    "\n",
    "    time_4 = time.time()\n",
    "\n",
    "    print('predicting cost ', time_4 - time_3, ' second', '\\n')\n",
    "\n",
    "\n",
    "\n",
    "    score = accuracy_score(test_y, test_predict)\n",
    "\n",
    "    print(\"The accruacy socre is \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
